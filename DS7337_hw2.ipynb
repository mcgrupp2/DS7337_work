{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "### DS7337 - 404\n",
    "\n",
    "Jason Rupp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from urllib import request\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_vocab_size(text):\n",
    "    # split words on white space and normalize case\n",
    "    sv_words = re.findall('\\w+', text.lower())\n",
    "    # unique words\n",
    "    sv_uniq_words = set(sv_words)\n",
    "    # create a df with the length of each word\n",
    "    sv_word_lengths = pd.DataFrame([len(w) for w in sv_uniq_words])\n",
    "    \n",
    "    # reshape the 1D df for the scaling function\n",
    "    sv_word_lengths.values.reshape(-1,1)\n",
    "    # define scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit data to scaler\n",
    "    scaler.fit(sv_word_lengths)\n",
    "    # transform data \n",
    "    df_rtn = scaler.transform(sv_word_lengths)\n",
    "    \n",
    "    # return scaler object\n",
    "    return(df_rtn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will will accept the raw text from the book, split on the words using regex. The text will have the case normalized by putting words in all lower case, then the unique words will be identified and put in a list. This unique word list will be iterated over to find the length of each word, creating a new list composed of the unique word lengths.\n",
    "\n",
    "The list with the word lengths for the unique words will be scaled using min/max scaling using `MinMaxScaler()`. This function is from the scikit-learn package and is what was suggested for this exercise, the reference guide can be found <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\">here.</a> The method by which the scaling is accomplished is shown below:\n",
    "\n",
    "\n",
    "$$X_{unscaled} = \\frac{(X - min(X))}{(max(X) - min(X))}$$\n",
    "\n",
    "***\n",
    "\n",
    "$$X_{scaled} = X_{unscaled} * (max + min) + min$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_long_vocab(text):\n",
    "    # split words on white space and normalize case\n",
    "    slv_words = re.findall('\\w+', text.lower())\n",
    "    # unique words\n",
    "    slv_uniq_words = set(slv_words)\n",
    "    # create a list with each word over length 13\n",
    "    slv_long_word = [w for w in slv_uniq_words if len(w) > 13]\n",
    "    # count the length of each word and put in a new list\n",
    "    slv_long_vSize = [len(i) for i in slv_long_word]\n",
    "    # scale the list of long word lengths\n",
    "    longScaler = minmax_scale(slv_long_vSize)\n",
    "        \n",
    "    return(longScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function operates in a very similar manner to the first scaling function. Again the method will be passed the text, split the words using regex, case normalize word, then identify the unique words and subset to words over length 13. Then this list of long unique words will be iterated over to find the length of each word, creating a new list composed of the unique word lengths. The value of 13 was chosen by elimination of other candidates. The book example gave 15 characters, however when 14, 15 were attempted, very few were returned.\n",
    "\n",
    "The list of long unique words will be scaled using min/max scaling using a slightly different but equivalent scikit-learn function; `minmax_scale()`. This is very similar to what was suggested for this exercise, the reference guide can be found <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html\">here.</a> The method by which the scaling is accomplished the same as the formula above, in question 1, however `minmax_scale()` can accommodate one dimensional data without the need to reshape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    ld_words = re.findall('\\w+', text.lower())\n",
    "    ld_uniq_words = set(ld_words)\n",
    "    return(round(len(ld_uniq_words)/len(ld_words),4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.gutenberg.org/files/34605/34605-0.txt\",\n",
    "       \"https://www.gutenberg.org/cache/epub/34728/pg34728.txt\",\n",
    "       \"https://www.gutenberg.org/files/44804/44804-0.txt\"]\n",
    "\n",
    "gradeLevels = [\"9th Grade\", \"10th Grade\", \"11th Grade\"]\n",
    "\n",
    "bookTitles = ['Betty Lee, Freshman, by Harriet Pyne Grove',\n",
    "             'Betty Lee, Sophomore, by Harriet Pyne Grove',\n",
    "             'Betty Lee, Junior, by Harriet Pyne Grove']\n",
    "\n",
    "normalizedWordCount = []\n",
    "normalizedLongWordCount = []\n",
    "lexDevs = []\n",
    "text_diff = []\n",
    "\n",
    "for x in range(3):\n",
    "    response = request.urlopen(urls[x])\n",
    "    raw = response.read().decode('utf8')\n",
    "    nwc = scaled_vocab_size(raw)\n",
    "    nlwc = scaled_long_vocab(raw)\n",
    "    ld = lexical_diversity(raw)\n",
    "    normalizedWordCount.append(sum(nwc)[0])\n",
    "    normalizedLongWordCount.append(sum(nlwc))\n",
    "    lexDevs.append(ld)\n",
    "    text_diff.append(\n",
    "        sum(nwc)[0] * sum(nlwc) * (ld) \n",
    "    )\n",
    "    \n",
    "colNames = [\"Grade Level\", \n",
    "            \"Book Title\", \n",
    "            \"Normalized Word Sum\", \n",
    "            \"Normalized Long Word Sum\", \n",
    "            \"Lexical Diversity\",\n",
    "            \"Text Difficulty Score\"\n",
    "           ]\n",
    "\n",
    "allDat = pd.DataFrame(\n",
    "    list(zip(gradeLevels, bookTitles, normalizedWordCount, normalizedLongWordCount, lexDevs, text_diff)),\n",
    "    columns = colNames\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade Level</th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Normalized Word Sum</th>\n",
       "      <th>Normalized Long Word Sum</th>\n",
       "      <th>Lexical Diversity</th>\n",
       "      <th>Text Difficulty Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9th Grade</td>\n",
       "      <td>Betty Lee, Freshman, by Harriet Pyne Grove</td>\n",
       "      <td>1815.600</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.090</td>\n",
       "      <td>574.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10th Grade</td>\n",
       "      <td>Betty Lee, Sophomore, by Harriet Pyne Grove</td>\n",
       "      <td>1716.667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>902.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Betty Lee, Junior, by Harriet Pyne Grove</td>\n",
       "      <td>1799.600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>770.229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade Level                                   Book Title  \\\n",
       "0   9th Grade   Betty Lee, Freshman, by Harriet Pyne Grove   \n",
       "1  10th Grade  Betty Lee, Sophomore, by Harriet Pyne Grove   \n",
       "2  11th Grade     Betty Lee, Junior, by Harriet Pyne Grove   \n",
       "\n",
       "   Normalized Word Sum  Normalized Long Word Sum  Lexical Diversity  \\\n",
       "0             1815.600                       3.5              0.090   \n",
       "1             1716.667                       6.0              0.088   \n",
       "2             1799.600                       5.0              0.086   \n",
       "\n",
       "   Text Difficulty Score  \n",
       "0                574.456  \n",
       "1                902.280  \n",
       "2                770.229  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round(allDat, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the results from the text difficulty scoring exercise, with 3 of the texts used in homework 1, along with each measure that was used in the calculation. The formula used to calculate the text difficulty is shown below.\n",
    "\n",
    "$$td\\ = \\Sigma{(nwc)}\\ *\\ \\Sigma{(nlwc)}\\ * ld$$\n",
    "\n",
    "This was a simple multiplication of the values. For the both lists of normalized word sums, shown in the formula as nwc and nlwc, the entire list was summed prior to multiplication.\n",
    "\n",
    "Calculating the text difficulty score on these three text from the first homework brought to light some other pieces of data. The normalized word count sum will take into account the lengths of the words and will be adjusted accordingly. The first homework asked a very similar question of does the vocab count give insight into difficulty, one of the very issues with that method was the fact that the unique words could be small and very simple. This method takes it into account. \n",
    "\n",
    "The score generated by the text difficulty function shown above does seem to correlate better with the idea that reading/vocabulary will become more difficult and advanced as one progresses through the years. There isn't a steady increase of scores, but there is a very obvious jump between the scores for the 9th grade and 11th grade readings, even though the 10th grade book complicates this slightly. What is admirable about the text difficulty score is that there seems to now be a much greater resolution between the levels. When comparing the lexical diversity alone, the numbers are very small, and very close together. For instance, the 9th grade text has the highest normalized word count and lexical diversity, but we can see from the long word score, that it had the least amount of words longer than 13. The text difficulty score seems to provide possibly a better measure than what was previously used.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
